{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad3ba50",
   "metadata": {},
   "source": [
    "# 1. テキストデータのTF-IDFベクトル化関数\n",
    "問題:\n",
    "複数の商品説明文（文字列のリスト）を受け取り、scikit-learn の TfidfVectorizer を使用して各文書をTF-IDFベクトルに変換する関数 vectorize_texts_tfidf(texts) を作成してください。\n",
    "ベクトル化の際には、一般的な日本語のストップワード（例: 「の」「です」「ます」など。簡単なリストで可）を除外し、トークン化は単語単位（スペース区切り、または簡易的な形態素解析ライブラリ利用を想定するが、ここでは簡易的にスペース区切りで良い）で行うこととします。\n",
    "戻り値は、TF-IDF行列（疎行列または密行列）と、使用したベクトライザのインスタンスとします。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 簡易的な日本語ストップワードリストの例\n",
    "japanese_stopwords = [\"の\", \"に\", \"は\", \"を\", \"た\", \"が\", \"で\", \"て\", \"と\", \"し\", \"れ\", \"さ\", \"ある\", \"いる\", \"も\", \"する\", \"から\", \"な\", \"こと\", \"として\", \"です\", \"ます\"]\n",
    "\n",
    "def vectorize_texts_tfidf(texts, stopwords=japanese_stopwords):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "descriptions = [\n",
    "    \"高機能 な スマートフォン です\",\n",
    "    \"軽量 で 高性能 な ノートパソコン\",\n",
    "    \"この スマートフォン は 最新 です\"\n",
    "]\n",
    "tfidf_matrix, vectorizer = vectorize_texts_tfidf(descriptions)\n",
    "# tfidf_matrix は (文書数 x 特徴語数) の行列\n",
    "# vectorizer.get_feature_names_out() などで特徴語を確認できる\n",
    "```\n",
    "注: 実際にはjanomeやMeCabといった形態素解析ライブラリと組み合わせることが望ましいですが、テストの簡略化のため上記のような想定としています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c109c",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae8aae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6230dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TF-IDF Vectorization Process ---\n",
      "\n",
      "--- TF-IDF Vectorization Results ---\n",
      "Number of documents: 3\n",
      "Number of features (vocabulary size): 7\n",
      "\n",
      "Features (vocabulary after stopword removal):\n",
      "['この' 'スマートフォン' 'ノートパソコン' '最新' '軽量' '高性能' '高機能']\n",
      "\n",
      "TF-IDF Matrix (dense representation for easy viewing):\n",
      "[[0.         0.60534851 0.         0.         0.         0.\n",
      "  0.79596054]\n",
      " [0.         0.         0.57735027 0.         0.57735027 0.57735027\n",
      "  0.        ]\n",
      " [0.62276601 0.4736296  0.         0.62276601 0.         0.\n",
      "  0.        ]]\n",
      "\n",
      "Detailed Document TF-IDF Vectors:\n",
      "\n",
      "Document 1: '高機能 な スマートフォン です'\n",
      "  高機能: 0.7960\n",
      "  スマートフォン: 0.6053\n",
      "\n",
      "Document 2: '軽量 で 高性能 な ノートパソコン'\n",
      "  ノートパソコン: 0.5774\n",
      "  軽量: 0.5774\n",
      "  高性能: 0.5774\n",
      "\n",
      "Document 3: 'この スマートフォン は 最新 です'\n",
      "  この: 0.6228\n",
      "  最新: 0.6228\n",
      "  スマートフォン: 0.4736\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 簡易的な日本語ストップワードリストの例（問題文に記載されているもの）\n",
    "japanese_stopwords = [\"の\", \"に\", \"は\", \"を\", \"た\", \"が\", \"で\", \"て\", \"と\", \"し\", \"れ\", \"さ\", \"ある\", \"いる\", \"も\", \"する\", \"から\", \"な\", \"こと\", \"として\", \"です\", \"ます\"]\n",
    "\n",
    "def vectorize_texts_tfidf(texts: list[str], stopwords: list[str] = japanese_stopwords):\n",
    "    \"\"\"\n",
    "    複数の商品説明文（文字列のリスト）を受け取り、scikit-learn の TfidfVectorizer を使用して\n",
    "    各文書をTF-IDFベクトルに変換します。\n",
    "\n",
    "    ベクトル化の際には、一般的な日本語のストップワードを除外し、トークン化は単語単位（スペース区切り）で行います。\n",
    "\n",
    "    Args:\n",
    "        texts (list[str]): 説明文のリスト。各文字列はスペースで区切られた単語と仮定します。\n",
    "        stopwords (list[str], optional): 除外する日本語のストップワードのリスト。\n",
    "                                         デフォルトは `japanese_stopwords` リスト。\n",
    "\n",
    "    Returns:\n",
    "        tuple: (tfidf_matrix, vectorizer_instance)\n",
    "            tfidf_matrix (scipy.sparse.csr_matrix): 計算されたTF-IDF行列。\n",
    "            vectorizer_instance (sklearn.feature_extraction.text.TfidfVectorizer): 使用したTfidfVectorizerのインスタンス。\n",
    "    \"\"\"\n",
    "    # TfidfVectorizerのインスタンスを作成します。\n",
    "    # stop_words: 指定されたストップワードリストを除外します。\n",
    "    # tokenizer: テキストをスペースで分割してトークンを生成するカスタム関数を指定します。\n",
    "    #            これにより、問題文の「簡易的にスペース区切りで良い」という要件を満たします。\n",
    "    # lowercase: デフォルトTrueですが、日本語では通常変更の必要はありません。\n",
    "    vectorizer = TfidfVectorizer(stop_words=stopwords, tokenizer=lambda text: text.split(' '))\n",
    "\n",
    "    # textsの各文書をTF-IDFベクトルに変換します。\n",
    "    # fit_transformメソッドは、語彙を学習し（fit）、その語彙に基づいて文書を変換（transform）します。\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # TF-IDF行列とベクトライザのインスタンスを返します。\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# 期待される動作例の確認\n",
    "if __name__ == '__main__':\n",
    "    descriptions = [\n",
    "        \"高機能 な スマートフォン です\",\n",
    "        \"軽量 で 高性能 な ノートパソコン\",\n",
    "        \"この スマートフォン は 最新 です\"\n",
    "    ]\n",
    "\n",
    "    print(\"--- TF-IDF Vectorization Process ---\")\n",
    "    tfidf_matrix, vectorizer = vectorize_texts_tfidf(descriptions)\n",
    "\n",
    "    print(\"\\n--- TF-IDF Vectorization Results ---\")\n",
    "    print(f\"Number of documents: {tfidf_matrix.shape[0]}\")\n",
    "    print(f\"Number of features (vocabulary size): {tfidf_matrix.shape[1]}\")\n",
    "\n",
    "    print(\"\\nFeatures (vocabulary after stopword removal):\")\n",
    "    # get_feature_names_out() で学習された特徴語（単語）のリストを取得します。\n",
    "    # ここにストップワードが含まれていないことを確認できます。\n",
    "    features = vectorizer.get_feature_names_out()\n",
    "    print(features)\n",
    "\n",
    "    print(\"\\nTF-IDF Matrix (dense representation for easy viewing):\")\n",
    "    # 疎行列を密行列に変換して表示します。（大規模データではメモリを大量消費するため注意）\n",
    "    print(tfidf_matrix.toarray())\n",
    "\n",
    "    # 各文書とそのTF-IDFベクトルをより詳細に確認する例\n",
    "    print(\"\\nDetailed Document TF-IDF Vectors:\")\n",
    "    for i, doc in enumerate(descriptions):\n",
    "        print(f\"\\nDocument {i+1}: '{doc}'\")\n",
    "        # 各文書のTF-IDFベクトル（疎行列の行）を密ベクトルに変換\n",
    "        doc_vector = tfidf_matrix[i].toarray().flatten()\n",
    "        \n",
    "        # 単語とTF-IDF値のペアを表示（値が0より大きいもののみ）\n",
    "        word_tfidf_scores = {word: score for word, score in zip(features, doc_vector) if score > 0}\n",
    "        # スコアを降順でソートして表示\n",
    "        sorted_scores = sorted(word_tfidf_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        for word, score in sorted_scores:\n",
    "            print(f\"  {word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa6093b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "    \"高機能 な スマートフォン です\",\n",
    "    \"軽量 で 高性能 な ノートパソコン\",\n",
    "    \"この スマートフォン は 最新 です\"\n",
    "]\n",
    "tfidf_matrix, vectorizer = vectorize_texts_tfidf(descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42548764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t0.7959605415681654\n",
      "  (0, 1)\t0.6053485081062917\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 5)\t0.5773502691896257\n",
      "  (1, 2)\t0.5773502691896257\n",
      "  (2, 1)\t0.4736296010332684\n",
      "  (2, 0)\t0.6227660078332259\n",
      "  (2, 3)\t0.6227660078332259\n",
      "TfidfVectorizer(stop_words=['の', 'に', 'は', 'を', 'た', 'が', 'で', 'て', 'と', 'し',\n",
      "                            'れ', 'さ', 'ある', 'いる', 'も', 'する', 'から', 'な', 'こと',\n",
      "                            'として', 'です', 'ます'],\n",
      "                tokenizer=<function vectorize_texts_tfidf.<locals>.<lambda> at 0x13a55a1f0>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6ac60d",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b5dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8863fa9",
   "metadata": {},
   "source": [
    "# 2. 簡単な商品カテゴリ分類モデルの訓練と評価\n",
    "問題:\n",
    "商品名（文字列のリスト X_texts）とそれに対応するカテゴリラベル（文字列のリスト y_labels）が与えられます。\n",
    "これらのデータを用いて、以下の手順で簡単な商品カテゴリ分類モデルを訓練し、評価する関数 train_and_evaluate_classifier(X_texts, y_labels) を作成してください。\n",
    "\n",
    "TfidfVectorizer を用いて商品名をベクトル化します。\n",
    "データを訓練用とテスト用に分割します（例: 80%訓練、20%テスト）。\n",
    "scikit-learn の LogisticRegression （または NaiveBayes）を用いて分類モデルを訓練します。\n",
    "テストデータでモデルの正解率（accuracy）を計算し、返します。\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression # または from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate_classifier(X_texts, y_labels):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "# サンプルデータ\n",
    "X_texts = [\"最新 iPhone 15 Pro\", \"美味しいリンゴ 青森産\", \"高性能ノートパソコン XYZ\", \"オーガニックコットン Tシャツ\", \"格安 Android スマホ\"]\n",
    "y_labels = [\"スマートフォン\", \"食品\", \"PC\", \"衣類\", \"スマートフォン\"]\n",
    "\n",
    "accuracy = train_and_evaluate_classifier(X_texts, y_labels)\n",
    "# accuracy は 0.0 から 1.0 の間の数値\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22a87ec",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8a12bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e600a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression # 指示通りLogisticRegressionを使用\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train_and_evaluate_classifier(X_texts, y_labels):\n",
    "    \"\"\"\n",
    "    商品名（文字列のリスト）とカテゴリラベル（文字列のリスト）を用いて、\n",
    "    簡単な商品カテゴリ分類モデルを訓練し、評価する関数。\n",
    "\n",
    "    Args:\n",
    "        X_texts (list[str]): 商品名のリスト。\n",
    "        y_labels (list[str]): 各商品名に対応するカテゴリラベルのリスト。\n",
    "\n",
    "    Returns:\n",
    "        float: モデルのテストデータに対する正解率（accuracy）。\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. TfidfVectorizer を用いて商品名をベクトル化します。\n",
    "    # TfidfVectorizerのインスタンスを作成\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # X_texts（商品名）をTF-IDF特徴量ベクトルに変換\n",
    "    X_vectorized = vectorizer.fit_transform(X_texts)\n",
    "\n",
    "    # 2. データを訓練用とテスト用に分割します（例: 80%訓練、20%テスト）。\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_vectorized, y_labels, test_size=0.2, random_state=42 # stratify=y_labels\n",
    "    )\n",
    "\n",
    "    # 3. scikit-learn の LogisticRegression を用いて分類モデルを訓練します。\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # 訓練データを用いてモデルを学習\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 4. テストデータでモデルの正解率（accuracy）を計算し、返します。\n",
    "    # テストデータで予測を実行\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 予測結果と実際のラベルを比較し、正解率を計算\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476615c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの正解率 (Accuracy): 0.00\n"
     ]
    }
   ],
   "source": [
    "# 期待される動作例: サンプルデータ\n",
    "X_texts = [\"最新 iPhone 15 Pro\", \"美味しいリンゴ 青森産\", \"高性能ノートパソコン XYZ\", \"オーガニックコットン Tシャツ\", \"格安 Android スマホ\"]\n",
    "y_labels = [\"スマートフォン\", \"食品\", \"PC\", \"衣類\", \"スマートフォン\"]\n",
    "\n",
    "# 関数を呼び出し、結果を出力\n",
    "accuracy = train_and_evaluate_classifier(X_texts, y_labels)\n",
    "print(f\"モデルの正解率 (Accuracy): {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f1e03",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b6276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9760707",
   "metadata": {},
   "source": [
    "# 3. コサイン類似度計算関数\n",
    "問題:\n",
    "2つの特徴ベクトル（NumPy配列）vec1 と vec2 を受け取り、それらのコサイン類似度を計算する関数 calculate_cosine_similarity(vec1, vec2) を作成してください。\n",
    "コサイン類似度は、ベクトルの内積をそれぞれのベクトルのL2ノルム（ユークリッドノルム）の積で割った値です。ゼロ除算が発生する場合は0を返すようにしてください。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "vec_a = np.array([1, 1, 0, 1, 0])\n",
    "vec_b = np.array([1, 0, 1, 1, 1])\n",
    "vec_c = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "similarity_ab = calculate_cosine_similarity(vec_a, vec_b) # 約0.577\n",
    "similarity_ac = calculate_cosine_similarity(vec_a, vec_c) # 0.0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4f8969",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb02aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    2つの特徴ベクトル（NumPy配列）のコサイン類似度を計算します。\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray): 1つ目の特徴ベクトル。\n",
    "        vec2 (np.ndarray): 2つ目の特徴ベクトル。\n",
    "\n",
    "    Returns:\n",
    "        float: 計算されたコサイン類似度。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. ベクトルの内積を計算します。\n",
    "    # np.dot() は2つのNumPy配列の内積を計算するのに使われます。\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "\n",
    "    # 2. それぞれのベクトルのL2ノルム（ユークリッドノルム）を計算します。\n",
    "    # np.linalg.norm() はベクトルのノルムを計算するのに使われます。\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "\n",
    "    # 3. ゼロ除算が発生しないようにチェックします。\n",
    "    # どちらかのベクトルのノルムが0（つまりゼロベクトル）の場合、\n",
    "    # そのベクトルは方向を持たないため、コサイン類似度を定義できません。\n",
    "    # この問題の要件に従い、この場合は0を返します。\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        # 4. コサイン類似度の計算式を適用します。\n",
    "        # 内積をノルムの積で割る\n",
    "        cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
    "        return float(cosine_similarity) # 念のためfloat型に変換\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b61311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "vec_a = np.array([1, 1, 0, 1, 0])\n",
    "vec_b = np.array([1, 0, 1, 1, 1])\n",
    "vec_c = np.array([0, 0, 0, 0, 0])\n",
    "\n",
    "similarity_ab = calculate_cosine_similarity(vec_a, vec_b) # 約0.577\n",
    "similarity_ac = calculate_cosine_similarity(vec_a, vec_c) # 0.0\n",
    "\n",
    "\n",
    "print(similarity_ab)\n",
    "print(similarity_ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f874c8",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb608e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edb0fe3c",
   "metadata": {},
   "source": [
    "# 4. K-Meansクラスタリングの適用とクラスタ中心の取得\n",
    "問題:\n",
    "商品データ（各商品が複数の数値特徴を持つと仮定し、NumPy配列 X_features で与えられる）とクラスタ数 k を受け取り、scikit-learn の KMeans を用いてデータをクラスタリングする関数 perform_kmeans_clustering(X_features, k) を作成してください。\n",
    "この関数は、各データポイントが属するクラスタのラベル（配列）と、各クラスタの中心座標（配列）を返すものとします。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def perform_kmeans_clustering(X_features, k):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "# サンプルデータ (商品数 x 特徴数)\n",
    "X_features = np.array([\n",
    "    [1, 2], [1.5, 1.8], [5, 8],\n",
    "    [8, 8], [1, 0.6], [9, 11]\n",
    "])\n",
    "k = 2\n",
    "cluster_labels, cluster_centers = perform_kmeans_clustering(X_features, k)\n",
    "# cluster_labels は各データ点がどのクラスタに属するかを示す配列 (例: [0, 0, 1, 1, 0, 1])\n",
    "# cluster_centers は各クラスタの中心座標を示す配列 (例: [[1.16, 1.46], [7.33, 9.0]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f6e75b",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b36083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def perform_kmeans_clustering(X_features, k):\n",
    "    \"\"\"\n",
    "    scikit-learn の KMeans を用いでデータをクラスタリングする\n",
    "\n",
    "    Args:\n",
    "        X_features (np.ndarray): 商品データ。各行が1つの商品、各列が特徴を表す。\n",
    "        形状は (n_samples, n_features)\n",
    "        \n",
    "    Returns:\n",
    "        tpule:\n",
    "            - cluster_labels (np.ndarray): 各データポイントが属するクラスタのラベル。\n",
    "            形状は(n_samples, )\n",
    "            - cluster_centers (np.ndarray): 各クラスタの中心座標。\n",
    "            形状は(k, n_features)\n",
    "    \"\"\"\n",
    "    # KMeansモデル\n",
    "    kmeans_model = KMeans(n_clusters=k, random_state=42, n_init='auto') \n",
    "    # データにモデルを適合\n",
    "    kmeans_model.fit(X_features)\n",
    "    # クラスタのラベルを取得\n",
    "    cluster_labels = kmeans_model.labels_\n",
    "    # クラスタの中心座標を取得\n",
    "    cluster_centers = kmeans_model.cluster_centers_\n",
    "    \n",
    "    return cluster_labels, cluster_centers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2cc140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各データポイントのクラスタラベル:\n",
      "[1 1 0 0 1 0]\n",
      "各クラスタの中心座標:\n",
      "[[7.33333333 9.        ]\n",
      " [1.16666667 1.46666667]]\n"
     ]
    }
   ],
   "source": [
    "# サンプルデータ (商品数 x 特徴数)\n",
    "X_features = np.array([\n",
    "    [1, 2], [1.5, 1.8], [5, 8],\n",
    "    [8, 8], [1, 0.6], [9, 11]\n",
    "])\n",
    "k = 2\n",
    "\n",
    "# 関数を実行\n",
    "cluster_labels, cluster_centers = perform_kmeans_clustering(X_features, k)\n",
    "\n",
    "print(\"各データポイントのクラスタラベル:\")\n",
    "print(cluster_labels)\n",
    "\n",
    "print(\"各クラスタの中心座標:\")\n",
    "print(cluster_centers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e9687",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c4009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733bf707",
   "metadata": {},
   "source": [
    "# 5. 混同行列からの評価指標計算関数\n",
    "問題:\n",
    "ある二値分類モデルのテスト結果として得られた混同行列（2x2のNumPy配列）が与えられます。この混同行列から、正解率（Accuracy）、適合率（Precision）、再現率（Recall）、F1スコアを計算する関数 calculate_metrics_from_confusion_matrix(cm) を作成してください。\n",
    "混同行列の形式は [[TN, FP], [FN, TP]] （TN: True Negative, FP: False Positive, FN: False Negative, TP: True Positive）とします。ゼロ除算が発生する場合は0を返すようにしてください。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics_from_confusion_matrix(cm):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "# 例: TN=50, FP=10, FN=5, TP=100\n",
    "confusion_matrix = np.array([[50, 10], [5, 100]])\n",
    "metrics = calculate_metrics_from_confusion_matrix(confusion_matrix)\n",
    "# metrics は {'accuracy': accuracy_val, 'precision': precision_val, 'recall': recall_val, 'f1_score': f1_score_val} のような辞書\n",
    "# 期待値: accuracy 約0.909, precision 約0.909, recall 約0.952, f1_score 約0.930\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c54928c",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79e0e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_metrics_from_confusion_matrix(cm):\n",
    "    \"\"\"\n",
    "    混同行列から正解率、適合率、再現率、F1スコアを計算する。\n",
    "\n",
    "    Args:\n",
    "        cm (np.ndarray): 2x2のNumPy配列で表された混同行列。\n",
    "                         形式は [[TN, FP], [FN, TP]]。\n",
    "\n",
    "    Returns:\n",
    "        dict: 各評価指標を含む辞書。\n",
    "              {'accuracy': accuracy_val, 'precision': precision_val, \n",
    "               'recall': recall_val, 'f1_score': f1_score_val}\n",
    "              ゼロ除算が発生する場合は、該当する指標の値は0となる。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 混同行列から各要素を抽出\n",
    "    TN = cm[0, 0]\n",
    "    FP = cm[0, 1]\n",
    "    FN = cm[1, 0]\n",
    "    TP = cm[1, 1]\n",
    "\n",
    "    # --- 1. 正解率 (Accuracy) の計算 ---\n",
    "    # 全てのサンプル数\n",
    "    total_samples = TP + TN + FP + FN\n",
    "    if total_samples == 0:\n",
    "        accuracy = 0.0\n",
    "    else:\n",
    "        accuracy = (TP + TN) / total_samples\n",
    "\n",
    "    # --- 2. 適合率 (Precision) の計算 ---\n",
    "    # 正と予測された総数\n",
    "    predicted_positives = TP + FP\n",
    "    if predicted_positives == 0:\n",
    "        precision = 0.0\n",
    "    else:\n",
    "        precision = TP / predicted_positives\n",
    "\n",
    "    # --- 3. 再現率 (Recall) の計算 ---\n",
    "    # 実際に正である総数\n",
    "    actual_positives = TP + FN\n",
    "    if actual_positives == 0:\n",
    "        recall = 0.0\n",
    "    else:\n",
    "        recall = TP / actual_positives\n",
    "\n",
    "    # --- 4. F1スコア (F1 Score) の計算 ---\n",
    "    # Precision と Recall の合計が0の場合（両方0の場合）もゼロ除算になるため注意\n",
    "    if precision + recall == 0:\n",
    "        f1_score = 0.0\n",
    "    else:\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    # 結果を辞書にまとめて返す\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b520f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 例1 ---\n",
      "混同行列:\n",
      "[[ 50  10]\n",
      " [  5 100]]\n",
      "計算結果: {'accuracy': 0.9090909090909091, 'precision': 0.9090909090909091, 'recall': 0.9523809523809523, 'f1_score': 0.9302325581395349}\n",
      "\n",
      "--- 全ての要素が0の混同行列 ---\n",
      "混同行列:\n",
      "[[0 0]\n",
      " [0 0]]\n",
      "計算結果: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "\n",
      "--- 完全な分類（TPのみ） ---\n",
      "混同行列:\n",
      "[[  0   0]\n",
      " [  0 100]]\n",
      "計算結果: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}\n",
      "\n",
      "--- 全て負と予測され、実際も全て負（TNのみ） ---\n",
      "混同行列:\n",
      "[[100   0]\n",
      " [  0   0]]\n",
      "計算結果: {'accuracy': 1.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "\n",
      "--- 全て誤って正と予測（FPのみ） ---\n",
      "混同行列:\n",
      "[[ 0 10]\n",
      " [ 0  0]]\n",
      "計算結果: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
      "\n",
      "--- 全て誤って負と予測（FNのみ） ---\n",
      "混同行列:\n",
      "[[0 0]\n",
      " [5 0]]\n",
      "計算結果: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# --- 期待される動作例の検証 ---\n",
    "print(\"--- 例1 ---\")\n",
    "# 例: TN=50, FP=10, FN=5, TP=100\n",
    "confusion_matrix1 = np.array([[50, 10], [5, 100]])\n",
    "metrics1 = calculate_metrics_from_confusion_matrix(confusion_matrix1)\n",
    "print(f\"混同行列:\\n{confusion_matrix1}\")\n",
    "print(f\"計算結果: {metrics1}\")\n",
    "# 期待値: accuracy 約0.909, precision 約0.909, recall 約0.952, f1_score 約0.930\n",
    "# 実際: {'accuracy': 0.9090909090909091, 'precision': 0.9090909090909091, 'recall': 0.9523809523809523, 'f1_score': 0.9302325581395349}\n",
    "\n",
    "\n",
    "# --- その他のテストケース ---\n",
    "print(\"\\n--- 全ての要素が0の混同行列 ---\")\n",
    "# TN=0, FP=0, FN=0, TP=0\n",
    "confusion_matrix_all_zeros = np.array([[0, 0], [0, 0]])\n",
    "metrics_all_zeros = calculate_metrics_from_confusion_matrix(confusion_matrix_all_zeros)\n",
    "print(f\"混同行列:\\n{confusion_matrix_all_zeros}\")\n",
    "print(f\"計算結果: {metrics_all_zeros}\")\n",
    "# 期待値: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "\n",
    "print(\"\\n--- 完全な分類（TPのみ） ---\")\n",
    "# TN=0, FP=0, FN=0, TP=100\n",
    "confusion_matrix_perfect_tp = np.array([[0, 0], [0, 100]])\n",
    "metrics_perfect_tp = calculate_metrics_from_confusion_matrix(confusion_matrix_perfect_tp)\n",
    "print(f\"混同行列:\\n{confusion_matrix_perfect_tp}\")\n",
    "print(f\"計算結果: {metrics_perfect_tp}\")\n",
    "# 期待値: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1_score': 1.0}\n",
    "\n",
    "print(\"\\n--- 全て負と予測され、実際も全て負（TNのみ） ---\")\n",
    "# TN=100, FP=0, FN=0, TP=0\n",
    "confusion_matrix_perfect_tn = np.array([[100, 0], [0, 0]])\n",
    "metrics_perfect_tn = calculate_metrics_from_confusion_matrix(confusion_matrix_perfect_tn)\n",
    "print(f\"混同行列:\\n{confusion_matrix_perfect_tn}\")\n",
    "print(f\"計算結果: {metrics_perfect_tn}\")\n",
    "# 期待値: {'accuracy': 1.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "\n",
    "print(\"\\n--- 全て誤って正と予測（FPのみ） ---\")\n",
    "# TN=0, FP=10, FN=0, TP=0\n",
    "confusion_matrix_only_fp = np.array([[0, 10], [0, 0]])\n",
    "metrics_only_fp = calculate_metrics_from_confusion_matrix(confusion_matrix_only_fp)\n",
    "print(f\"混同行列:\\n{confusion_matrix_only_fp}\")\n",
    "print(f\"計算結果: {metrics_only_fp}\")\n",
    "# 期待値: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}\n",
    "\n",
    "print(\"\\n--- 全て誤って負と予測（FNのみ） ---\")\n",
    "# TN=0, FP=0, FN=5, TP=0\n",
    "confusion_matrix_only_fn = np.array([[0, 0], [5, 0]])\n",
    "metrics_only_fn = calculate_metrics_from_confusion_matrix(confusion_matrix_only_fn)\n",
    "print(f\"混同行列:\\n{confusion_matrix_only_fn}\")\n",
    "print(f\"計算結果: {metrics_only_fn}\")\n",
    "# 期待値: {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1_score': 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c95ae",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2289e09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b57273a2",
   "metadata": {},
   "source": [
    "# 6. 特徴量の標準化関数\n",
    "問題:\n",
    "数値特徴からなるデータセット（NumPy配列 X、各行がサンプル、各列が特徴量）を受け取り、各特徴量を標準化（平均0、標準偏差1にスケーリング）する関数 standardize_features(X) を作成してください。scikit-learn の StandardScaler を使用するか、NumPyで直接計算しても構いません。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler # 使っても良い\n",
    "\n",
    "def standardize_features(X):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "X_original = np.array([[1, -1, 2],\n",
    "                       [2, 0, 0],\n",
    "                       [0, 1, -1]], dtype=float)\n",
    "X_standardized = standardize_features(X_original)\n",
    "# X_standardized の各列は平均がほぼ0、標準偏差がほぼ1になる\n",
    "# 例:\n",
    "# [[ 0.         -1.22474487  1.33630621]\n",
    "#  [ 1.22474487  0.         -0.26726124]\n",
    "#  [-1.22474487  1.22474487 -1.06904497]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59505ab5",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa394c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb132f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4a74dc4",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775dbcd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb1da0a",
   "metadata": {},
   "source": [
    "# 7. LightGBM/XGBoostの主要ハイパーパラメータ設定の理解\n",
    "問題:\n",
    "LightGBMまたはXGBoostのような勾配ブースティング木モデルにおいて、モデルの複雑さを制御し、過学習を抑制するために調整されることが多い主要なハイパーパラメータを3つ挙げ、それぞれのパラメータがモデルにどのような影響を与えるかを簡単に説明してください。そして、それらのパラメータを指定してモデルを初期化する（訓練は不要）簡単なPythonコードスニペットを lightgbm.LGBMClassifier または xgboost.XGBClassifier を用いて示してください。\n",
    "\n",
    "期待される説明とコード例:\n",
    "\n",
    "例として挙げるパラメータ:\n",
    "n_estimators (木の数): 増やすとモデルの表現力は上がるが、過学習しやすくなる。\n",
    "learning_rate (学習率): 小さいほど学習は慎重に進み、汎化性能が上がることがあるが、多くの木が必要になる。\n",
    "max_depth (木の深さ): 深いほど複雑な関係を学習できるが、過学習しやすくなる。\n",
    "（その他、num_leaves, min_child_samples, subsample, colsample_bytree なども候補）\n",
    "\n",
    "```Python\n",
    "# LightGBMの場合の例\n",
    "import lightgbm as lgb\n",
    "\n",
    "def initialize_lgbm_with_params(n_estimators, learning_rate, max_depth):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42 # 再現性のため\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 使用例\n",
    "lgbm_model = initialize_lgbm_with_params(n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "print(lgbm_model.get_params())\n",
    "```\n",
    "\n",
    "注: この問題はコーディングそのものより、ハイパーパラメータの知識を問うものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd6af9",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeca64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225974e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a05b8c7",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d85bc3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f8ca831",
   "metadata": {},
   "source": [
    "# 8. 不均衡データに対する簡易オーバーサンプリング関数\n",
    "問題:\n",
    "二値分類タスクにおいて、少数派クラスのデータが極端に少ない不均衡データセット（特徴量 X とラベル y、NumPy配列）が与えられたとします。少数派クラスのサンプルを単純に複製することでオーバーサンプリングを行う関数 simple_oversample(X, y) を作成してください。多数派クラスのサンプル数は変更せず、少数派クラスのサンプル数が多数派クラスのサンプル数と同じになるように複製するものとします。\n",
    "（imblearn ライブラリの RandomOverSampler のような高度なものではなく、基本的な動作を実装してください。）\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def simple_oversample(X, y):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "# サンプルデータ (特徴量は1次元で簡略化)\n",
    "X_imbalanced = np.array([[1],[2],[3],[4],[5],[6],[7],[8],[9],[10]])\n",
    "y_imbalanced = np.array([0, 0, 1, 0, 0, 0, 1, 0, 0, 0]) # クラス1が少数派\n",
    "\n",
    "X_resampled, y_resampled = simple_oversample(X_imbalanced, y_imbalanced)\n",
    "# Counter(y_resampled) の結果、両クラスのサンプル数が等しくなる (この例ではクラス0が8件、クラス1が2件なので、クラス1を6件複製し合計16件、各8件)\n",
    "# X_resampled の形状と y_resampled の形状も確認\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d29e06",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4f638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c652e89d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8daecb7b",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f91098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edea8435",
   "metadata": {},
   "source": [
    "# 9. 商品レビューの簡易感情分析（ポジネガ辞書利用）\n",
    "問題:\n",
    "商品レビューのテキストと、ポジティブ単語のセット positive_words、ネガティブ単語のセット negative_words を受け取り、レビューの感情スコアを計算する関数 calculate_sentiment_score(review_text, positive_words, negative_words) を作成してください。\n",
    "スコアは「レビューに含まれるポジティブ単語の数 - レビューに含まれるネガティブ単語の数」とします。単語のカウントは単純な出現回数とし、大文字・小文字は区別しないものとします。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "def calculate_sentiment_score(review_text, positive_words, negative_words):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "positive_set = {\"良い\", \"素晴らしい\", \"満足\", \"最高\", \"便利\"}\n",
    "negative_set = {\"悪い\", \"残念\", \"不満\", \"ひどい\", \"壊れた\"}\n",
    "\n",
    "review1 = \"この商品はとても良い。素晴らしい！\"\n",
    "score1 = calculate_sentiment_score(review1, positive_set, negative_set) # 期待値: 2 (良い, 素晴らしい)\n",
    "\n",
    "review2 = \"期待外れで残念。少し悪い点もある。\"\n",
    "score2 = calculate_sentiment_score(review2, positive_set, negative_set) # 期待値: -2 (残念, 悪い)\n",
    "\n",
    "review3 = \"特にコメントなし。\"\n",
    "score3 = calculate_sentiment_score(review3, positive_set, negative_set) # 期待値: 0\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9034c7",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9374b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e012fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bed5387e",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485fe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1f4306",
   "metadata": {},
   "source": [
    "# 10. 時系列データからの移動平均計算関数\n",
    "問題:\n",
    "商品の売上データなどの時系列データ（数値のリストまたはNumPy配列 series）と、ウィンドウサイズ window_size を受け取り、移動平均を計算してリストまたはNumPy配列で返す関数 calculate_moving_average(series, window_size) を作成してください。\n",
    "計算結果の配列の長さは、元の時系列データから window_size - 1 を引いたものになります（ウィンドウ内のデータが揃わない先頭部分は計算しない）。\n",
    "\n",
    "期待される動作例:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "\n",
    "def calculate_moving_average(series, window_size):\n",
    "    # ここに処理を記述\n",
    "    pass\n",
    "\n",
    "sales_data = np.array([10, 12, 11, 15, 16, 14, 18, 20, 19, 22])\n",
    "window = 3\n",
    "moving_avg = calculate_moving_average(sales_data, window)\n",
    "# 期待される出力 (numpy array): [11.        , 12.66666667, 14.        , 15.        , 16.        , 17.33333333, 19.        , 20.33333333]\n",
    "# ( (10+12+11)/3, (12+11+15)/3, ... )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a30a1",
   "metadata": {},
   "source": [
    "## 回答"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa751b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b33d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f901228",
   "metadata": {},
   "source": [
    "# 練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9726106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
